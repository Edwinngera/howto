https://www.youtube.com/watch?v=UeF_N1r91RQ



####################################################
#
#	Aptana dont work on JAVA10
#
#	https://www.eclipse.org/pdt/  # eclipse-php-oxygen-3-linux-gtk-x86_64.tar.gz  # java 10 ?
#	https://github.com/aptana/studio3/releases  # java 9 ?
#	https://www.eclipse.org/downloads/download.php?file=/oomph/epp/oxygen/R2/eclipse-inst-linux64.tar.gz
#
####################################################

java --version
openjdk 10.0.1 2018-04-17
OpenJDK Runtime Environment (build 10.0.1+10-Ubuntu-3ubuntu1)
OpenJDK 64-Bit Server VM (build 10.0.1+10-Ubuntu-3ubuntu1, mixed mode)

https://wiki.eclipse.org/Configure_Eclipse_for_Java_9
https://github.com/aptana/studio3/issues/464
https://blog.takipi.com/5-features-in-java-9-that-will-change-how-you-develop-software-and-2-that-wont/
https://www.heise.de/developer/meldung/Java-10-ist-fertig-und-im-Zeitplan-3999905.html






####################################################
#
# PHP Frameworks Install 
#
####################################################

sudo apt install composer
sudo apt install curl
sudo apt install php-intl -y
sudo apt install php-xml php-mbstring -y
sudo apt install php-sqlite3
sudo apt install apache2
sudo apt install php-mysql
sudo a2dismod php5.6 ; sudo a2enmod php7.0 ; sudo service apache2 restart

sudo apt-get remove --purge apache2  php7.0 libapache2-mod-php
sudo apt-get install apache2 php7.0 libapache2-mod-php
sudo service apache2 restart


sudo a2dismod mpm_event
sudo a2enmod mpm_prefork
udo a2dismod mpm_prefork
sudo a2dismod mpm_worker
sudo a2dismod mpm_event


sudo ln -s ~/Git/ /var/www/html/Git

http://localhost/Git/


---------------------------------------------------
cakephp 3.x
---------------------------------------------------
https://book.cakephp.org/3.0/en/installation.html
https://book.cakephp.org/3.0/en/tutorials-and-examples/cms/installation.html
https://book.cakephp.org/3.0/en/tutorials-and-examples/blog/blog.html
https://book.cakephp.org/3.0/en/tutorials-and-examples/bookmarks/intro.html
https://book.cakephp.org/3.0/en/tutorials-and-examples.html#
https://book.cakephp.org/3.0/en/tutorials-and-examples/bookmarks/part-two.html
https://book.cakephp.org/3.0/en/installation.html


#composer create-project --prefer-dist cakephp/app my_app_name
composer create-project --prefer-dist cakephp/app bookmarker
composer create-project --prefer-dist cakephp/app blog

composer self-update && composer create-project --prefer-dist cakephp/app blog

$average_of_foo = array_sum($foo) / count($foo); 

bin/cake bake all users
bin/cake bake all bookmarks
bin/cake bake all tags

bin/cake server
bin/cake server -H 192.168.13.37 -p 5673

http://localhost/Git/bookmarker/index.php/bookmarks
http://localhost/Git/adminer-4.6.2.php?username=root&db=cake_bookmarks


---------------------------------------------------
symfony 3.8
---------------------------------------------------
https://github.com/symfony/demo
https://symfony.com/doc/current/setup.html
https://symfony.com/doc/current/page_creation.html
https://github.com/symfony/demo

composer create-project symfony/symfony-demo

php bin/console server:run
php bin/console server:start 0.0.0.0:8000

https://symfony.com/doc/current/setup/web_server_configuration.html
https://symfony.com/doc/current/setup.html
https://symfony.com/doc/current/page_creation.html

http://localhost/Git/symfony-demo/web/app_dev.php/en/blog/
http://localhost/Git/symfony-demo/web/app_dev.php/en/blog/posts/abnobas-sunt-hilotaes-de-placidus-vita


---------------------------------------------------
codeigniter 3.X
---------------------------------------------------
https://codeigniter.com
https://codeigniter.com/user_guide/installation/downloads.html


---------------------------------------------------
laravel 5.6
---------------------------------------------------
https://laravel.com
https://laravel.com/docs/5.6
https://laravel.com/docs/5.6/controllers
http://laraveldaily.com/create-controller-model-one-artisan-command/
https://www.tutorialspoint.com/laravel/laravel_controllers.htm

composer global require "laravel/installer"
composer create-project --prefer-dist laravel/laravel blog

php artisan serve

php artisan make:controller PhotoController --resource
php artisan make:controller PhotoController --resource --model=Photo
php artisan make:controller API/PhotoController --api
php artisan route:cache
php artisan route:clear

php artisan help make:controller
php artisan make:controller [Name]Controller
php artisan make:controller --plain <controller name>
php artisan make:model ModelName -m -cr  #   generate Model , controller with resources and migration
php artisan krlove:generate:model Videos --table-name=videos

make
  make:command         Create a new command class
  make:console         Create a new Artisan command
  make:controller      Create a new resource controller class
  make:event           Create a new event class
  make:middleware      Create a new middleware class
  make:migration       Create a new migration file
  make:model           Create a new Eloquent model class
  make:provider        Create a new service provider class
  make:request         Create a new form request class




---------------------------------------------------
lumen
---------------------------------------------------
https://lumen.laravel.com/docs/5.6
composer global require "laravel/lumen-installer"
composer create-project --prefer-dist laravel/lumen blog
php -S localhost:8000 -t public


---------------------------------------------------
slim
---------------------------------------------------
https://www.slimframework.com/docs/
composer require slim/slim "^3.0"


---------------------------------------------------
silex
---------------------------------------------------
https://silex.symfony.com/doc/2.0/
composer require silex/silex "^2.0"
composer create-project fabpot/silex-skeleton path/to/install " ^2.0"


---------------------------------------------------
phalconphp
---------------------------------------------------
https://docs.phalconphp.com/en/3.3/installation

Phalcon need the following extensions to run (minimal):

curl
gettext
gd2 (to use the Phalcon\Image\Adapter\Gd class)
libpcre3-dev (Debian/Ubuntu), pcre-devel (CentOS), pcre (macOS)
json
mbstring
pdo_*
fileinfo
openssl

curl -s https://packagecloud.io/install/repositories/phalcon/stable/script.deb.sh | sudo bash

sudo apt-get update
sudo apt-get install php7.0-phalcon
sudo add-apt-repository ppa:ondrej/php
sudo apt-get update
sudo apt-get install php-phalcon

git clone https://github.com/phalcon/cphalcon
cd cphalcon/build
sudo ./install

cd cphalcon/build

# One of the following:
sudo ./install --arch 32bits
sudo ./install --arch 64bits
sudo ./install --arch safe

php -r 'print_r(get_loaded_extensions());'  # check extensions 
php -m  # check modules


















####################################################
#
#	kdnuggets tutorials Juni 2018
#
####################################################
---------------------------------------------------
https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score as acc
from mlxtend.feature_selection import SequentialFeatureSelector as sfs

# Read data
df = pd.read_csv('winequality-white.csv', sep=';')

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    df.values[:,:-1],
    df.values[:,-1:],
    test_size=0.25,
    random_state=42)

y_train = y_train.ravel()
y_test = y_test.ravel()

print('Training dataset shape:', X_train.shape, y_train.shape)
print('Testing dataset shape:', X_test.shape, y_test.shape)

# Build RF classifier to use in feature selection
clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)

# Build step forward feature selection
sfs1 = sfs(clf,
           k_features=5,
           forward=True,
           floating=False,
           verbose=2,
           scoring='accuracy',
           cv=5)

# Perform SFFS
sfs1 = sfs1.fit(X_train, y_train)

# Which features?
feat_cols = list(sfs1.k_feature_idx_)
print(feat_cols)

# Build full model with selected features
clf = RandomForestClassifier(n_estimators=1000, random_state=42, max_depth=4)
clf.fit(X_train[:, feat_cols], y_train)

y_train_pred = clf.predict(X_train[:, feat_cols])
print('Training accuracy on selected features: %.3f' % acc(y_train, y_train_pred))

y_test_pred = clf.predict(X_test[:, feat_cols])
print('Testing accuracy on selected features: %.3f' % acc(y_test, y_test_pred))

# Build full model on ALL features, for comparison
clf = RandomForestClassifier(n_estimators=1000, random_state=42, max_depth=4)
clf.fit(X_train, y_train)

y_train_pred = clf.predict(X_train)
print('Training accuracy on all features: %.3f' % acc(y_train, y_train_pred))

y_test_pred = clf.predict(X_test)
print('Testing accuracy on all features: %.3f' % acc(y_test, y_test_pred))


* * * * * * * * * * * * * * * * * * * * 

https://www.kdnuggets.com/2018/06/taming-lstms-variable-sized-mini-batches-pytorch.html
https://gist.github.com/williamFalcon/f75b01b47bf1be8b1e80be173ebdf39c#file-bilstm_net_design-py
https://gist.github.com/williamFalcon/b0dc6d25b39e7da0d05e5713ef0a57af#file-pytorch_lstm_forward-py
https://gist.github.com/williamFalcon/42da07d5cea5d00151f9cfde30f092b6#file-pytorch_lstm_loss-py
https://gist.github.com/williamFalcon/f27c7b90e34b4ba88ced042d9ef33edd#file-pytorch_lstm_variable_mini_batches-py
* * * * * * * * * * * * * * * * * * * * 
https://www.kdnuggets.com/2018/06/generating-text-rnn-4-lines-code.html
https://github.com/minimaxir/textgenrnn
http://www.trumptwitterarchive.com/archive

from textgenrnn import textgenrnn
textgen = textgenrnn()
textgen.train_from_file('trump-tweets.txt', num_epochs=10)
textgen.generate(5)

textgen.generate(5, temperature=0.9)
textgen.generate(5, temperature=0.1)

* * * * * * * * * * * * * * * * * * * * 

https://www.kdnuggets.com/2018/06/natural-language-semantic-search-arbitrary-objects-deep-learning.html
https://www.kdnuggets.com/2018/06/better-stats-101.html


# 1. Live Loss Plot

from livelossplot import PlotLossesKeras
model.fit(X_train, Y_train,
          epochs=10,
          validation_data=(X_test, Y_test),
          callbacks=[PlotLossesKeras()],
          verbose=0)


# 2. Parfit
from parfit import bestFit # Necessary if you wish to use bestFit
# Necessary if you wish to run each step sequentially
from parfit.fit import *
from parfit.score import *
from parfit.plot import *
from parfit.crossval import *
grid = {
    'min_samples_leaf': [1, 5, 10, 15, 20, 25],
    'max_features': ['sqrt', 'log2', 0.5, 0.6, 0.7],
    'n_estimators': [60],
    'n_jobs': [-1],
    'random_state': [42]
}
paramGrid = ParameterGrid(grid)
best_model, best_score, all_models, all_scores = bestFit(RandomForestClassifier(), paramGrid,
                                                    X_train, y_train, X_val, y_val, # nfolds=5 [optional, instead of validation set]
                                                    metric=roc_auc_score, greater_is_better=True, 
                                                    scoreLabel='AUC')

print(best_model, best_score)

# 4. textgenrnn
from textgenrnn import textgenrnn
textgen = textgenrnn()
textgen.train_from_file('hacker-news-2000.txt', num_epochs=1)
textgen.generate()

# 5. Magnitude
from pymagnitude import *
vectors = Magnitude("/path/to/vectors.magnitude")

* * * * * * * * * * * * * * * * * * * * 

https://www.kdnuggets.com/2018/06/packaging-distributing-python-project-pypi-pip.html

1. Creating a Simple Python Project.
2. How Python Locates Libraries?
3. Manual Installation by Copying Project Files to site-packages.
4. How Python Installers Locate Libraries?
5. Preparing the Package and its Files (__init__.py and setup.py).
6. Distributing the Package.
7. Uploading the Distribution Files Online to Test PyPI.
8. Installing the Distributed Package from Test PyPI.
9. Importing and Using the Installed Package.
10. Using PyPI rather than Test PyPI.

* * * * * * * * * * * * * * * * * * * * 

https://www.kdnuggets.com/2018/06/command-line-tricks-data-scientists.html

ICONV
HEAD
TR
WC
SPLIT
SORT & UNIQ
CUT
PASTE
JOIN
GREP
SED
AWK

# Converting -f (from) latin1 (ISO-8859-1)
# -t (to) standard UTF_8
iconv -f ISO-8859-1 -t UTF-8 < input.txt > output.txt

# Print first 3 lines
head -n 3 filename.csv

# Converting a tab delimited file into commas
cat tab_delimited.txt | tr "\\t" "," comma_delimited.csv
cat README.md | tr "[:punct:][:space:]" "\n" | tr "[:upper:]" "[:lower:]" | grep . | sort | uniq -c | sort -nr
# Converting all upper case letters to lower case
cat filename.csv | tr '[A-Z]' '[a-z]'


Useful options:
tr -d delete characters
tr -s squeeze characters
\b backspace
\f form feed
\v vertical tab
\NNN character with octal value NNN

# Will return number of lines in CSV
wc -l gigantic_comma.csv

# We will split our CSV into new_filename every 500 line
split -l 500 filename.csv new_filename_
find . -type f -exec mv '{}' '{}'.csv \;

Useful options:
split -b split by certain byte size
split -a generate suffixes of length N
split -x split using hex suffixes

SORT & UNIQ
# Sorting a CSV file by the second column alphabetically
sort -t, -k2 filename.csv
# Numerically
sort -t, -k2n filename.csv
# Reverse order
sort -t, -k2nr filename.csv

Useful options:
sort -f ignore case
sort -r reverse sort order
sort -R scramble order
uniq -c count number of occurrences
uniq -d only print duplicate lines

CUT
# get  the first and third columns
cut -d, -f 1,3 filename.csv

# select every column other than the first.
cut -d, -f 2- filename.csv

# Print first 10 lines of column 1 and 3, where "some_string_value" is present
head filename.csv | grep "some_string_value" | cut -d, -f 1,3


Finding out the number of unique values within the second column.
cat filename.csv | cut -d, -f 2 | sort | uniq | wc -l

# Count occurences of unique values, limiting to first 10 results
cat filename.csv | cut -d, -f 2 | sort | uniq -c | head

PASTE

# Join the two into a CSV
paste -d ',' names.txt jobs.txt > person_data.txt

# Output
adam,lawyer
john,youtuber
zach,developer


JOIN
# Join the first file (-1) by the second column
# and the second file (-2) by the first
join -t, -1 2 -2 1 first_file.txt second_file.txt

# Outer join, replace blanks with NULL in columns 1 and 2
# -o which fields to substitute - 0 is key, 1.1 is first column, etc...
join -t, -1 2 -a 1 -a2 -e ' NULL' -o '0,1.1,2.2' first_file.txt second_file.txt

Useful options:
join -a print unpairable lines
join -e replace missing input fields
join -j equivalent to -1 FIELD -2 FIELD

GREP
# Recursively search and list all files in directory containing 'word'
grep -lr 'word' .

# List number of files containing word
grep -lr 'word' . | wc -l
grep -c 'some_value' filename.csv

# Same thing, but in all files in current directory by file name
grep -c 'some_value' *
grep "first_value\|second_value" filename.csv

Useful options

alias grep="grep --color=auto" make grep colorful
grep -E use extended regexps
grep -w only match whole words
grep -l print name of files with match
grep -v inverted matching

SED
sed -i '' 's/\$//g' data.txt
sed -i '' 's/\([0-9]\),\([0-9]\)/\1\2/g' data.txt
sed -i '' '/jack/d' data.txt

AWK
awk '/word/' filename.csv
awk -F, '/word/ { print $3 "\t" $4 }' filename.csv
awk -F, 'NR == 53' filename.csv
awk -F, ' $1 == "string" { print NR, $0 } ' filename.csv

# Filter based off of numerical value in second column
awk -F, ' $2 == 1000 { print NR, $0 } ' filename.csv

# Print line number and columns where column three greater
# than 2005 and column five less than one thousand
awk -F, ' $3 >= 2005 && $5 <= 1000 { print NR, $0 } ' filename.csv

# Sum the third column:
awk -F, '{ x+=$3 } END { print x }' filename.csv

# sum of the third column, for values where the first column equals “something”.
awk -F, '$1 == "something" { x+=$3 } END { print x }' filename.csv

# Get the dimensions of a file:
awk -F, 'END { print NF, NR }' filename.csv

# Prettier version
awk -F, 'BEGIN { print "COLUMNS", "ROWS" }; END { print NF, NR }' filename.csv

# Print lines appearing twice:
awk -F, '++seen[$0] == 2' filename.csv

Remove duplicate lines:
# Consecutive lines
awk 'a !~ $0; {a=$0}']
# Nonconsecutive lines
awk '! a[$0]++' filename.csv
# More efficient
awk '!($0 in a) {a[$0];print}

# Substitute multiple values using built-in function gsub().
awk '{gsub(/scarlet|ruby|puce/, "red"); print}'

This awk command will combine multiple CSV files, ignoring the header and then append it at the end.
awk 'FNR==1 && NR!=1{next;}{print}' *.csv > final_file.csv


Need to downsize a massive file? Welp, awk can handle that with help from sed. Specifically, this command breaks one big file into multiple smaller ones based on a line count. This one-liner will also add an extension.
sed '1d;$d' filename.csv | awk 'NR%NUMBER_OF_LINES==1{x="filename-"++i".csv";}{print > x}'
# Example: splitting big_data.csv into data_(n).csv every 100,000 lines
sed '1d;$d' big_data.csv | awk 'NR%100000==1{x="data_"++i".csv";}{print > x}'


* * * * * * * * * * * * * * * * * * * * 

https://www.kdnuggets.com/2018/06/audience-segmentation.html

* * * * * * * * * * * * * * * * * * * * 

https://www.kdnuggets.com/2018/06/keras-4-step-workflow.html

# Define the training data
import numpy as np
X_train = np.random.random((5000, 32))
y_train = np.random.random((5000, 5))
# Define the neural network model
from keras import models
from keras import layers
INPUT_DIM = X_train.shape[1]
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_dim=INPUT_DIM))
model.add(layers.Dense(5, activation='softmax'))
# Configure the learning process
from keras import optimizers
from keras import metrics
model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
# Train the model
model.fit(X_train, y_train, 
          batch_size=128, 
          epochs=10)

* * * * * * * * * * * * * * * * * * * * 

https://www.kdnuggets.com/2018/06/linear-regression-predictive-modeling-r.html

data(trees) ## access the data from R’s datasets package
head(trees) ## look at the first several rows of the data
str(trees) ## look at the structure of the variables 
ggpairs(data=trees, columns=1:3, title="trees data")