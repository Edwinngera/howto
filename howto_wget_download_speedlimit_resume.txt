wget --limit-rate=100k url
wget --limit-rate=500k --continue url

Test connection speed:

curl -o /dev/null http://speedtest.sea01.softlayer.com/downloads/test10.zip
wget http://speedtest.wdc01.softlayer.com/downloads/test500.zip && del test500.zip

More examples:
http://www.linuxtechi.com/wget-command-practical-examples/
http://www.cyberciti.biz/tips/wget-resume-broken-download.html

---------------



$ wget --random-wait -r -p -e robots=off -U Mozilla \
    http://yoururl....

Downloads recursively the content of the url.

--random-wait - wait between 0.5 to 1.5 seconds between requests.
-r - turn on recursive retrieving.
-e robots=off - ignore robots.txt.
-U Mozilla - set the "User-Agent" header to "Mozilla". Though a better choice is a real User-Agent like "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729)".

Some other useful options are:

--limit-rate=20k - limits download speed to 20kbps.
-o logfile.txt - log the downloads.
-l 0 - remove recursion depth (which is 5 by default).
--wait=1h - be sneaky, download one file every hour.

