http://pythonfiddle.com/
http://rextester.com/l/python_online_compiler
http://www.compileonline.com/execute_python_online.php
https://ideone.com/2mkVbD
https://repl.it/repls/AgreeableAntiqueMouse
https://repl.it/repls/ExperiencedInsidiousSection
https://www.jdoodle.com/python3-programming-online
https://www.onlinegdb.com/online_python_compiler
https://www.onlinegdb.com/online_python_interpreter
https://www.python.org/shell/
https://www.pythonanywhere.com/try-ipython/
https://www.tutorialspoint.com/execute_python_online.php
https://quintagroup.com/cms/python/online-interpreter
https://www.pythonanywhere.com/try-ipython/
------------------------------------------------

nltk
https://www.pythonanywhere.com/try-ipython/
https://repl.it/repls/YellowGrowingComputing
------------------------------------------------
http://www.nltk.org/api/nltk.html?highlight=freqdist
https://www.nltk.org/book/ch01.html
https://www.strehle.de/tim/weblog/archives/2015/09/03/1569
https://pythonspot.com/nltk-stop-words/
------------------------------------------------


##################################################################################################

Training and Testing the Naive Bayes Classifier
https://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/

##################################################################################################

import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews

def word_feats(words):
    return dict([(word, True) for word in words])

negids = movie_reviews.fileids('neg')
posids = movie_reviews.fileids('pos')

negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]
posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]

negcutoff = len(negfeats)*3/4
poscutoff = len(posfeats)*3/4

trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]
testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]
print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))

classifier = NaiveBayesClassifier.train(trainfeats)
print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)
classifier.show_most_informative_features()



##################################################################################################

TEXT CLASSIFICATION FOR SENTIMENT ANALYSIS â€“ ELIMINATE LOW INFORMATION FEATURES
https://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/

##################################################################################################

import collections, itertools
import nltk.classify.util, nltk.metrics
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews, stopwords
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures
from nltk.probability import FreqDist, ConditionalFreqDist

def evaluate_classifier(featx):
    negids = movie_reviews.fileids('neg')
    posids = movie_reviews.fileids('pos')

    negfeats = [(featx(movie_reviews.words(fileids=[f])), 'neg') for f in negids]
    posfeats = [(featx(movie_reviews.words(fileids=[f])), 'pos') for f in posids]

    negcutoff = len(negfeats)*3/4
    poscutoff = len(posfeats)*3/4

    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]
    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]

    classifier = NaiveBayesClassifier.train(trainfeats)
    refsets = collections.defaultdict(set)
    testsets = collections.defaultdict(set)

    for i, (feats, label) in enumerate(testfeats):
            refsets[label].add(i)
            observed = classifier.classify(feats)
            testsets[observed].add(i)

    print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)
    print 'pos precision:', nltk.metrics.precision(refsets['pos'], testsets['pos'])
    print 'pos recall:', nltk.metrics.recall(refsets['pos'], testsets['pos'])
    print 'neg precision:', nltk.metrics.precision(refsets['neg'], testsets['neg'])
    print 'neg recall:', nltk.metrics.recall(refsets['neg'], testsets['neg'])
    classifier.show_most_informative_features()

def word_feats(words):
    return dict([(word, True) for word in words])

print 'evaluating single word features'
evaluate_classifier(word_feats)

word_fd = FreqDist()
label_word_fd = ConditionalFreqDist()

for word in movie_reviews.words(categories=['pos']):
    word_fd.inc(word.lower())
    label_word_fd['pos'].inc(word.lower())

for word in movie_reviews.words(categories=['neg']):
    word_fd.inc(word.lower())
    label_word_fd['neg'].inc(word.lower())

# n_ii = label_word_fd[label][word]
# n_ix = word_fd[word]
# n_xi = label_word_fd[label].N()
# n_xx = label_word_fd.N()

pos_word_count = label_word_fd['pos'].N()
neg_word_count = label_word_fd['neg'].N()
total_word_count = pos_word_count + neg_word_count

word_scores = {}

for word, freq in word_fd.iteritems():
    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],
        (freq, pos_word_count), total_word_count)
    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],
        (freq, neg_word_count), total_word_count)
    word_scores[word] = pos_score + neg_score

best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:10000]
bestwords = set([w for w, s in best])

def best_word_feats(words):
    return dict([(word, True) for word in words if word in bestwords])

print 'evaluating best word features'
evaluate_classifier(best_word_feats)

def best_bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):
    bigram_finder = BigramCollocationFinder.from_words(words)
    bigrams = bigram_finder.nbest(score_fn, n)
    d = dict([(bigram, True) for bigram in bigrams])
    d.update(best_word_feats(words))
    return d

print 'evaluating best words + bigram chi_sq word features'
evaluate_classifier(best_bigram_word_feats)




https://bitbucket.org/japerk/nltk-trainer/src
https://github.com/japerk/nltk-trainer
http://www.cs.cornell.edu/people/pabo/movie-review-data/
http://text-processing.com/demo/sentiment/
each method is throttled to 1000 calls per day per IP
http://text-processing.com/docs/sentiment.html

curl -d "text=terrible" http://text-processing.com/api/sentiment/
curl -d "text=great" http://text-processing.com/api/sentiment/
curl -d "text=hi friend" http://text-processing.com/api/sentiment/
curl -d "language=dutch&text=goed boek" http://text-processing.com/api/sentiment/













###################################################

SentimentAnalyzerSample

###################################################

https://repl.it/languages

https://www.nltk.org/book/ch05.html
https://www.kaggle.com/ngyptr/python-nltk-sentiment-analysis
https://www.programcreek.com/python/example/91264/nltk.sentiment
https://www.learndatasci.com/tutorials/sentiment-analysis-reddit-headlines-pythons-nltk/
https://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/
https://github.com/nagypeterjob/Sentiment-Analysis-NLTK-ML-LSTM/blob/master/Sentiment.ipynb
https://opensourceforu.com/2016/12/analysing-sentiments-nltk/
https://www.nltk.org/api/nltk.sentiment.html


https://github.com/Delvison/Sentiment-Analysis/blob/master/sentiment_analysis.py
https://github.com/sangeeth96/twitter-sentiment-analysis/blob/master/twitter.py
https://github.com/Jordan396/Basic_Twitter_Sentiment_Analysis/blob/master/main_NLTK.py
https://github.com/nisarg64/Sentiment-Analysis-NLTK/blob/master/imdb_sentiment.py
https://github.com/Bharat123rox/sentiment-analysis-nltk/blob/master/SentimentAnalysis/SentimentAnalyzerSample.py
https://github.com/alokkumary2j/Sentiment-Analysis-Using-Python-NLTK/blob/master/BayesSemanticAnalyzer.py
https://github.com/uditkumawat/Sentiment-Analysis-In-Python-using-nltk/blob/master/main.py
https://github.com/Kamaldeep-j/Sentiment-Analysis-Python-NLTK/blob/master/sentiment-analysis/test.py
https://github.com/PatGaston3/Sentiment-Analysis/blob/master/AnalyzeSentiment.py
https://github.com/maranemil/howto/blob/master/fullstack/python/nltk/nltk_replit2.py