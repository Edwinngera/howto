for z in *.csv; do zip -6 $z.zip $z ; done

------------------------------------------------------

sudo apt install composer

composer jsonlint php-cli php-cli-prompt php-common php-composer-ca-bundle
  php-composer-semver php-composer-spdx-licenses php-json-schema php-psr-log
  php-symfony-console php-symfony-filesystem php-symfony-finder
  php-symfony-polyfill-mbstring php-symfony-process php7.1-cli php7.1-common
  php7.1-json php7.1-opcache php7.1-readline

Creating config file /etc/php/7.1/mods-available/calendar.ini with new version
Creating config file /etc/php/7.1/mods-available/ctype.ini with new version
Creating config file /etc/php/7.1/mods-available/exif.ini with new version
Creating config file /etc/php/7.1/mods-available/fileinfo.ini with new version
Creating config file /etc/php/7.1/mods-available/ftp.ini with new version
Creating config file /etc/php/7.1/mods-available/gettext.ini with new version
Creating config file /etc/php/7.1/mods-available/iconv.ini with new version
Creating config file /etc/php/7.1/mods-available/pdo.ini with new version
Creating config file /etc/php/7.1/mods-available/phar.ini with new version
Creating config file /etc/php/7.1/mods-available/posix.ini with new version
Creating config file /etc/php/7.1/mods-available/shmop.ini with new version
Creating config file /etc/php/7.1/mods-available/sockets.ini with new version
Creating config file /etc/php/7.1/mods-available/sysvmsg.ini with new version
Creating config file /etc/php/7.1/mods-available/sysvsem.ini with new version
Creating config file /etc/php/7.1/mods-available/sysvshm.ini with new version
Creating config file /etc/php/7.1/mods-available/tokenizer.ini with new version

------------------------------------------------------

sudo apt install mysql-client mysql-server

mysql --version
mysql  Ver 14.14 Distrib 5.7.22, for Linux (x86_64) using  EditLine wrapper

------------------------------------------------------

sudo apt install mongodb mongodb-clients  mongodb-server mongo-tools
sudo apt install postgresql postgresql-client postgresql-client-common

sudo apt install apache2
sudo apt-get install apache2 libapache2-mod-php
sudo apt-get install php-mysql php-mcrypt php-mbstring php-mongodb
sudo systemctl restart apache2

------------------------------------------------------

get

phpMyAdmin-4.8.3-all-languages.zip

wget http://c758482.r82.cf2.rackcdn.com/sublime-text_build-3083_amd64.deb
sudo dpkg -i sublime-text_build-3083_amd64.deb



------------------------------------------------------



CREATE DATABASE datagovro;

mysql -u root -p -database datagovro

CREATE TABLE contracte (
	Castigator  VARCHAR(255) NULL,
	CastigatorCUI  VARCHAR(255) NULL,
	CastigatorTara  VARCHAR(255) NULL,
	CastigatorLocalitate  VARCHAR(255) NULL,
	CastigatorAdresa  VARCHAR(255) NULL,
	Tip  VARCHAR(255) NULL,
	TipContract  VARCHAR(255) NULL,
	TipProcedura  VARCHAR(255) NULL,
	AutoritateContractanta  VARCHAR(255) NULL,
	AutoritateContractantaCUI  VARCHAR(255) NULL,
	TipAC  VARCHAR(255) NULL,
	TipActivitateAC  VARCHAR(255) NULL,
	NumarAnuntAtribuire  VARCHAR(255) NULL,
	DataAnuntAtribuire  VARCHAR(255) NULL,
	TipIncheiereContract  VARCHAR(255) NULL,
	TipCriteriiAtribuire  VARCHAR(255) NULL,
	CuLicitatieElectronica  VARCHAR(255) NULL,
	NumarOfertePrimite  VARCHAR(255) NULL,
	Subcontractat  VARCHAR(255) NULL,
	NumarContract  VARCHAR(255) NULL,
	DataContract  VARCHAR(255) NULL,
	TitluContract  VARCHAR(255) NULL,
	Valoare  VARCHAR(255) NULL,
	Moneda  VARCHAR(255) NULL,
	ValoareRON  VARCHAR(255) NULL,
	ValoareEUR  VARCHAR(255) NULL,
	CPVCodeID  VARCHAR(255) NULL,
	CPVCode  VARCHAR(255) NULL,
	NumarAnuntParticipare  VARCHAR(255) NULL,
	DataAnuntParticipare  VARCHAR(255) NULL,
	ValoareEstimataParticipare  VARCHAR(255) NULL,
	MonedaValoareEstimataParticipare  VARCHAR(255) NULL,
	FonduriComunitare  VARCHAR(255) NULL,
	TipFinantare  VARCHAR(255) NULL,
	TipLegislatieID  VARCHAR(255) NULL,
	FondEuropean  VARCHAR(255) NULL,
	ContractPeriodic  VARCHAR(255) NULL,
	DepoziteGarantii  VARCHAR(255) NULL,
	ModalitatiFinantare  VARCHAR(255) NULL
);





LOAD DATA LOCAL INFILE 'contracte-2007.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2008.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2009.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2010.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2011.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2012-s1.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2012-s2.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2013-s1.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2013-s2.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2014-s1.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2014-s2.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2015-s1.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2015-s2.csv' INTO TABLE contracte FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'contracte-2016-s1.csv' INTO TABLE contracte FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n';




ALTER TABLE `contracte` ADD `id` INT NOT NULL AUTO_INCREMENT FIRST, ADD PRIMARY KEY (`id`);

mysqldump -u root -p datagovro contracte > contracte.sql
mysqldump --compatible=postgresql -u root -p  datagovro contracte > contracte_postgresql.sql
zip contracte.sql.zip contracte.sql
zip contracte_postgresql.sql.zip contracte_postgresql.sql

-------------------------------------


CREATE TABLE cumparari_directe (

	Castigator VARCHAR(255) NULL,
	CastigatorCUI VARCHAR(255) NULL,
	CastigatorTara VARCHAR(255) NULL,
	CastigatorLocalitate VARCHAR(255) NULL,
	CastigatorAdresa VARCHAR(255) NULL,
	TipProcedura VARCHAR(255) NULL,
	AutoritateContractanta VARCHAR(255) NULL,
	AutoritateContractantaCU VARCHAR(255) NULL,
	NumarAnunt VARCHAR(255) NULL,
	DataAnunt VARCHAR(255) NULL,
	TipIncheiereContract VARCHAR(255) NULL,
	NumarContract VARCHAR(255) NULL,
	DataContract VARCHAR(255) NULL,
	TitluContract VARCHAR(255) NULL,
	Valoare VARCHAR(255) NULL,
	Moneda VARCHAR(255) NULL,
	ValoareRON VARCHAR(255) NULL,
	ValoareEUR VARCHAR(255) NULL,
	CPVCodeID VARCHAR(255) NULL,
	CPVCode VARCHAR(255) NULL

);


LOAD DATA LOCAL INFILE 'cumparari-directe-2007-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2007-t2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2007-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2007-t4.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2008-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2008-t2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2008-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2008-t4.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2009-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2009-t2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2009-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2009-t4.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2010-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2010-t2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2010-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2010-t4.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2011-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2011-t2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2011-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2011-t4.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2012-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2012-t2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2012-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2012-t4-part1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2012-t4-part2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2013-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2013-t2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2013-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2013-t4-part1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2013-t4-part2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2013-t4-part3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2014-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2014-t2-part1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2014-t2-part2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2014-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2014-t4-part1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2014-t4-part2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2015-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2015-t2-part-1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2015-t2-part-2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2015-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2015-t4.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2016-t1.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2016-t2.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';
LOAD DATA LOCAL INFILE 'cumparari-directe-2016-t3.csv' INTO TABLE cumparari_directe FIELDS TERMINATED BY '^' LINES TERMINATED BY '\n';


ALTER TABLE `cumparari_directe` ADD `id` INT NOT NULL AUTO_INCREMENT FIRST, ADD PRIMARY KEY (`id`);


mysqldump -u root -p datagovro cumparari_directe > cumparari_directe.sql
mysqldump --compatible=postgresql -u root -p  datagovro cumparari_directe > cumparari_directe_postgresql.sql

zip cumparari_directe.sql.zip cumparari_directe.sql
zip cumparari_directe_postgresql.sql.zip cumparari_directe_postgresql.sql


// IMPORT

TRUNCATE TABLE `contracte` ;

mysql -u root -p
use datagovro;
source contracte.sql;


// EXPORT 2 CSV
SELECT * INTO OUTFILE '/var/lib/mysql-files/contracte.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"' LINES TERMINATED BY '\n' FROM contracte WHERE 1;

SELECT @@GLOBAL.secure_file_priv;
+---------------------------+
| @@GLOBAL.secure_file_priv |
+---------------------------+
| /var/lib/mysql-files/     |
+---------------------------+
1 row in set (0.02 sec)


mysql> SELECT * FROM contracte INTO OUTFILE '/var/lib/mysql-files/contracte.csv' FIELDS TERMINATED BY ',';
Query OK, 855704 rows affected (18.64 sec)

sudo cp /var/lib/mysql-files/contracte.csv .


for z in *.zip; do unzip $z; done


----------------------------

##########################################################
#
#	postgres
#
###########################################################

sudo apt-get install pgadmin3

sudo -u postgres psql postgres
psql (9.6.9)
Type "help" for help.

postgres=# \password postgres
Enter new password:
Enter it again:
postgres=# create database datagovro;
CREATE DATABASE

\q # exit

pgadmin3


-- Database: datagovro
-- DROP DATABASE datagovro;

CREATE DATABASE datagovro
  WITH OWNER = postgres
       ENCODING = 'UTF8'
       TABLESPACE = pg_default
       LC_COLLATE = 'de_DE.UTF-8'
       LC_CTYPE = 'de_DE.UTF-8'
       CONNECTION LIMIT = -1;




/*
CREATE TABLE contracte (
	Castigator  VARCHAR(255) NULL,
	CastigatorCUI  VARCHAR(255) NULL,
	CastigatorTara  VARCHAR(255) NULL,
	CastigatorLocalitate  VARCHAR(255) NULL,
	CastigatorAdresa  VARCHAR(255) NULL,
	Tip  VARCHAR(255) NULL,
	TipContract  VARCHAR(255) NULL,
	TipProcedura  VARCHAR(255) NULL,
	AutoritateContractanta  VARCHAR(255) NULL,
	AutoritateContractantaCUI  VARCHAR(255) NULL,
	TipAC  VARCHAR(255) NULL,
	TipActivitateAC  VARCHAR(255) NULL,
	NumarAnuntAtribuire  VARCHAR(255) NULL,
	DataAnuntAtribuire  VARCHAR(255) NULL,
	TipIncheiereContract  VARCHAR(255) NULL,
	TipCriteriiAtribuire  VARCHAR(255) NULL,
	CuLicitatieElectronica  VARCHAR(255) NULL,
	NumarOfertePrimite  VARCHAR(255) NULL,
	Subcontractat  VARCHAR(255) NULL,
	NumarContract  VARCHAR(255) NULL,
	DataContract  VARCHAR(255) NULL,
	TitluContract  VARCHAR(255) NULL,
	Valoare  VARCHAR(255) NULL,
	Moneda  VARCHAR(255) NULL,
	ValoareRON  VARCHAR(255) NULL,
	ValoareEUR  VARCHAR(255) NULL,
	CPVCodeID  VARCHAR(255) NULL,
	CPVCode  VARCHAR(255) NULL,
	NumarAnuntParticipare  VARCHAR(255) NULL,
	DataAnuntParticipare  VARCHAR(255) NULL,
	ValoareEstimataParticipare  VARCHAR(255) NULL,
	MonedaValoareEstimataParticipare  VARCHAR(255) NULL,
	FonduriComunitare  VARCHAR(255) NULL,
	TipFinantare  VARCHAR(255) NULL,
	TipLegislatieID  VARCHAR(255) NULL,
	FondEuropean  VARCHAR(255) NULL,
	ContractPeriodic  VARCHAR(255) NULL,
	DepoziteGarantii  VARCHAR(255) NULL,
	ModalitatiFinantare  VARCHAR(255) NULL
);*/


-- Table: public.contracte
-- DROP TABLE public.contracte;

CREATE TABLE public.contracte
(
  castigator character varying(255),
  castigatorcui character varying(255),
  castigatortara character varying(255),
  castigatorlocalitate character varying(255),
  castigatoradresa character varying(255),
  tip character varying(255),
  tipcontract character varying(255),
  tipprocedura character varying(255),
  autoritatecontractanta character varying(255),
  autoritatecontractantacui character varying(255),
  tipac character varying(255),
  tipactivitateac character varying(255),
  numaranuntatribuire character varying(255),
  dataanuntatribuire character varying(255),
  tipincheierecontract character varying(255),
  tipcriteriiatribuire character varying(255),
  culicitatieelectronica character varying(255),
  numaroferteprimite character varying(255),
  subcontractat character varying(255),
  numarcontract character varying(255),
  datacontract character varying(255),
  titlucontract character varying(255),
  valoare character varying(255),
  moneda character varying(255),
  valoareron character varying(255),
  valoareeur character varying(255),
  cpvcodeid character varying(255),
  cpvcode character varying(255),
  numaranuntparticipare character varying(255),
  dataanuntparticipare character varying(255),
  valoareestimataparticipare character varying(255),
  monedavaloareestimataparticipare character varying(255),
  fonduricomunitare character varying(255),
  tipfinantare character varying(255),
  tiplegislatieid character varying(255),
  fondeuropean character varying(255),
  contractperiodic character varying(255),
  depozitegarantii character varying(255),
  modalitatifinantare character varying(255),
  id integer
)
WITH (
  OIDS=FALSE
);
ALTER TABLE public.contracte
  OWNER TO postgres;



SELECT * FROM contracte;
ALTER TABLE public.contracte ADD COLUMN id integer;





######################################################################


https://www.postgresql.org/docs/8.3/static/sql-copy.html
https://www.postgresql.org/docs/9.4/static/sql-copy.html
https://wiki.postgresql.org/wiki/Error_logging_in_COPY

######################################################################

mysql -u root -p
use datagovro;
SELECT * FROM contracte INTO OUTFILE '/var/lib/mysql-files/contracte1.csv' FIELDS TERMINATED BY '|';
exit;
sudo mv /var/lib/mysql-files/contracte1.csv .


#iconv  -t utf-8 contracte1.csv > contracte2.csv
#iconv -f ISO-8859-1 -t UTF-8 contracte1.csv > contracte2.csv
iconv -f ASCII -t utf-8//IGNORE contracte1.csv > contracte2.csv



#clear all non-ascii chars of file.txt
iconv -c -f utf-8 -t ascii contracte2.csv
strings contracte2.csv



sudo -u postgres psql postgres
\connect datagovro;
#SET client_encoding = 'ISO_8859_5';
\COPY contracte FROM 'contracte.csv' delimiter '|' csv;
\COPY contracte FROM 'contracte.csv' delimiter '|' csv ENCODING 'WIN1252';
\COPY contracte FROM 'contracte.csv' ( format csv, header, delimiter '|', encoding 'win1252', null '/N' );
\COPY contracte FROM 'contracte2.csv' WITH DELIMITER '|' CSV HEADER
\COPY contracte FROM 'contracte2.csv' WITH (FORMAT CSV, HEADER, DELIMITER E'|', NULL '\n');

\copy contracte (castigator, castigatorcui, castigatortara, castigatorlocalitate, castigatoradresa, tip, tipcontract, tipprocedura, autoritatecontractanta, autoritatecontractantacui, tipac, tipactivitateac, numaranuntatribuire, dataanuntatribuire, tipincheierecontract, tipcriteriiatribuire, culicitatieelectronica, numaroferteprimite, subcontractat, numarcontract, datacontract, titlucontract, valoare, moneda, valoareron, valoareeur, cpvcodeid, cpvcode, numaranuntparticipare, dataanuntparticipare, valoareestimataparticipare, monedavaloareestimataparticipare, fonduricomunitare, tipfinantare, tiplegislatieid, fondeuropean, contractperiodic, depozitegarantii, modalitatifinantare) FROM 'contracte2.csv' (FORMAT csv, HEADER, DELIMITER '|');


\COPY contracte FROM 'contracte2.csv' WITH (FORMAT CSV, HEADER, DELIMITER E'|', NULL '', ERROR_LOGGING_SKIP_BAD_ROWS);
\copy contracte from 'contracte2.csv'  with delimiter as '|' quote '"'  csv HEADER  ;


# COPY foo2 TO STDOUT WITH CSV;
# COPY foo2 FROM STDIN WITH NULL AS '';


ERROR:  option "error_logging_skip_bad_rows" not recognized
ERROR:  extra data after last expected column
ERROR:  missing data for column "modalitatifinantare"

----------------------
######################################################################
#
#	https://www.dataquest.io/blog/loading-data-into-postgres/
#
######################################################################

pip install psycopg2

import psycopg2
conn = psycopg2.connect("host=localhost dbname=postgres user=postgres")

import psycopg2
conn = psycopg2.connect("host=localhost dbname=postgres user=postgres")
cur = conn.cursor()
cur.execute('SELECT * FROM notes')
one = cur.fetchone()
all = cur.fetchall()

import psycopg2
conn = psycopg2.connect("host=localhost dbname=postgres user=postgres")
cur = conn.cursor()
insert_query = "INSERT INTO users VALUES {}".format("(10, 'hello@dataquest.io', 'Some Name', '123 Fake St.')")
cur.execute(insert_query)
conn.commit()

---------------------------------------------------------------------------------------------------------------

import csv
import psycopg2

conn = psycopg2.connect("host=localhost dbname=postgres user=postgres")
cur = conn.cursor()

with open('user_accounts.csv', 'r') as f:
    reader = csv.reader(f)
    next(reader)  # Skip the header row.
    for row in reader:
        cur.execute(
            "INSERT INTO users VALUES (%s, %s, %s, %s)",
            row
        )
conn.commit()

---------------------------------------------------------------------------------------------------------------
import psycopg2

conn = psycopg2.connect("host=localhost dbname=postgres user=postgres")
cur = conn.cursor()
with open('user_accounts.csv', 'r') as f:
    # Notice that we don't need the `csv` module.
    next(f)  # Skip the header row.
    cur.copy_from(f, 'users', sep=',')

conn.commit()

######################################################################

######################################################################

You can use awk to skip fields and create an intermediate file or
better yet, just pipe the output to copy. Here is a trivial example:

awk '{ FS = "\t" ; OFS = "\t" ; print $1,$3 }' inputdatafile

This sets the input and output field separators to tab and outputs the
first and third fields from inputdatafile. If you want to skip the
first record just add the following if statement:

awk '{ FS = "\t" ; OFS = "\t" ; if ( NR > 1 ) print $1,$3 }'
inputdatafile

######################################################################
#
# Perl
#
######################################################################


# Start copying some data to PostgeSQL. Here, STDIN is relative to the
# script. So the PostgreSQL STDIN being mentioned is actually just the
# stuff being written over $dbh->func( ..., 'putline');
$dbh->func( 'COPY some_table FROM STDIN', 'putline' );

# Write whatever was read from $fh to PostgreSQL's COPY.
while ( <$fh> )
{
    chomp;
    $dbh->func( $_, 'putline' );
}

# Tell PostgreSQL that the COPY is over with.
$dbh->func( "\\.\n", 'putline' );

######################################################################

http://php.net/manual/en/function.pg-copy-from.php

######################################################################

$db = pg_connect("dbname=publisher") or die("Could not connect");
$rows = pg_copy_to($db, $table_name);
pg_query($db, "DELETE FROM $table_name");
pg_copy_from($db, $table_name, $rows);


https://github.com/chanmix51/Pomm/archive/1.3.0-RC1.zip
https://github.com/chanmix51/Pomm
http://www.pomm-project.org/
https://github.com/pomm-project
https://www.pgadmin.org/
https://wiki.postgresql.org/wiki/Apt
https://www.pgadmin.org/download/pgadmin-4-python-wheel/
https://www.adminer.org/de/
-------------------------------------------------------------------------------

https://wiki.postgresql.org/wiki/Error_logging_in_COPY

Error logging in COPY was a proposed feature developed by Aster Data against the PostgreSQL 9.0 code base. It was submitted and reviewed (1) but not accepted into the core product for that or any other version so far.

Variable name 	Description 	Default value
ERROR_LOGGING 	Enables error handling for COPY commands (when set to true). 	true
ERROR_LOGGING_SKIP_BAD_ROWS 	Enables the ability to skip malformed tuples that are encountered in COPY commands (when set to true). 	true
ERROR_LOGGING_MAX_ERRORS 	Maximum number of bad rows to log before stopping the COPY operation (0 means unlimited). 	0
ERROR_LOGGING_SCHEMA_NAME 	Schema name of the table where malformed tuples are inserted by the error logging module 	'public'
ERROR_LOGGING_TABLE_NAME 	Relation name where malformed tuples are inserted by the error logging module. The table is automatically created if it does not exist. 	'error_table'
ERROR_LOGGING_LABEL 	Optional label that is used to identify malformed tuples 	COPY command text
ERROR_LOGGING_KEY 	Optional key to identify malformed tuples 	Index of the tuple in the COPY stream














######################################################################
https://wesleytsai.io/2015/07/26/mongodb-server-directory-permission-denied/
https://docs.mongodb.com/manual/tutorial/enable-authentication/


sudo apt install php-mongodb
sudo apt install php7.1-dev

git clone https://github.com/mongodb/mongo-php-driver.git
$ cd mongo-php-driver
$ git submodule update --init


$ phpize
$ ./configure
$ make all
$ make test
$ sudo make install

# Installing shared extensions:     /usr/lib/php/20160303/

sudo systemctl restart apache2

sudo mkdir -p /data/db
sudo mkdir -p /data/db
ls -ld /data/db/
sudo chmod 0755 /data/db
sudo chown -R $USER /data/db
sudo chmod -R go+w /data/db

sudo services mongodb start
sudo services mongodb stop

mongod --config /etc/mongod.conf
storage:
  dbPath: /var/lib/mongodb





mongod --port 27017 --dbpath /data/db1

ll /var/lib/mongodb
mongod --dbpath /var/lib/mongodb

sudo mongod
chcon -R --reference=/var/lib/mongo /data/db

mongo
> show dbs

> use testdb
# switched to db testdb

db.updateUser("admin",{roles : ["userAdminAnyDatabase","userAdmin","readWrite","dbAdmin","clusterAdmin","readWriteAnyDatabase","dbAdminAnyDatabase"]});

db.createUser(
   {
     user: "mongo_admin",
     pwd: "mongo_admin",
     roles: [ "readWrite", "dbAdmin" ]
   }
)

/*use admin
db.createUser(
  {
    user: "root",
    pwd: "root",
    roles: [ { role: "admin", db: "testdb" } ]
  }
)*/


use admin
db.auth("root", "testdb" )

######################################################################

https://www.tutorialspoint.com/mongodb/mongodb_create_database.htm
https://www.guru99.com/top-20-mongodb-tools.html

http://www.phpmoadmin.com/
https://www.adminer.org/de/
https://www.adminer.org/de/drivers/
http://www.phpmoadmin.com/

https://github.com/iwind/rockmongo
https://github.com/mongo-express/mongo-express
https://github.com/nosqlclient/nosqlclient
https://github.com/mongodb-labs/shard-viz

http://php.net/manual/de/class.mongodb-driver-manager.php

$manager = new MongoDB\Driver\Manager("mongodb://localhost:27017");
var_dump($manager);


mongoimport -d datagovro -c contracte --type csv --file contracte2.csv --headerline

# Failed: read error on entry #280: line 281, column 421: bare " in non-quoted-field

sed 's/\[""\]//g' FILE-NAME

db.<collection>.remove({})

----------------------------------------------------------------------




##########################################################
Limitations of MongoDB
http://tech.tulentsev.com/2014/02/limitations-of-mongodb/
https://docs.mongodb.com/manual/reference/method/cursor.limit/
##########################################################

Max document size: 16 MB (we all knew this one, right?)
Max document nesting level: 100 (documents inside documents inside documents…)
Namespace is limited to ~123 chars (namespace is db_name + collection_name (or index_name))
DB name is limited to 64 chars
Default .ns file can store about 24000 namespaces (again, a namespace is referring to a collection or an index)
If you index some field, that field can’t contain more than 1024 bytes
Max 64 indexes per collection
Max 31 fields in a compound index
Max 12 nodes in a replica set
Max 7 voting nodes in a replica set
You can’t automatically rollback more than 300 MB of data. If you have more than this, manual invervention is needed.
On linux, one mongod instance can’t store more than 64 TB of data (128 TB without journal)
You can’t change shard key for a collection once it’s sharded.
You can’t change value of a shard key of a document.
aggregate/$sort produces error if sorting takes more than 10 percent of RAM.
You can’t use $or in 2d geo queries
Forbidden characters in field names: .$
Hashed index can’t be unique
Max connection number is hardcoded to 20k.



##########################################################

mongo crud

##########################################################


mongo
show dbs
use cars
db.cars.insert({name: "Audi", price: 52642})
db.cars.insert({name: "Mercedes", price: 57127})
db.cars.insert({name: "Skoda", price: 9000})
db.cars.insert({name: "Volvo", price: 29000})
db.cars.insert({name: "Bentley", price: 350000})
db.cars.insert({name: "Citroen", price: 21000})
db.cars.insert({name: "Hummer", price: 41400})
db.cars.insert({name: "Volkswagen", price: 21600})
db.cars.find()
db.cars.find().count()
db.cars.remove({})

db.system.namespaces.count()
db.stats()
db.runCommand("logRotate")

# collection will always keep just one hour's worth of data
db.clientEvents.ensureIndex( { "createDate": 1 }, { expireAfterSeconds: 3600 } )

exit



##########################################################

http://zetcode.com/db/mongodbphp/

##########################################################



# dbstats.php

<?php

try {

    $mng = new MongoDB\Driver\Manager("mongodb://localhost:27017");
    $stats = new MongoDB\Driver\Command(["dbstats" => 1]);
    $res = $mng->executeCommand("testdb", $stats);
    $stats = current($res->toArray());
    print_r($stats);

} catch (MongoDB\Driver\Exception\Exception $e) {

    $filename = basename(__FILE__);
    echo "The $filename script has experienced an error.\n";
    echo "It failed with the following exception:\n";
    echo "Exception:", $e->getMessage(), "\n";
    echo "In file:", $e->getFile(), "\n";
    echo "On line:", $e->getLine(), "\n";
}

?>


# list_databases.php

<?php

try {

    $mng = new MongoDB\Driver\Manager("mongodb://localhost:27017");
    $listdatabases = new MongoDB\Driver\Command(["listDatabases" => 1]);
    $res = $mng->executeCommand("admin", $listdatabases);
    $databases = current($res->toArray());
    foreach ($databases->databases as $el) {
        echo $el->name . "\n";
    }

} catch (MongoDB\Driver\Exception\Exception $e) {

    $filename = basename(__FILE__);
    echo "The $filename script has experienced an error.\n";
    echo "It failed with the following exception:\n";
    echo "Exception:", $e->getMessage(), "\n";
    echo "In file:", $e->getFile(), "\n";
    echo "On line:", $e->getLine(), "\n";
}

?>

read_all.php

<?php

try {

    $mng = new MongoDB\Driver\Manager("mongodb://localhost:27017");
    $query = new MongoDB\Driver\Query([]);
    $rows = $mng->executeQuery("testdb.cars", $query);
    foreach ($rows as $row) {
            echo "$row->name : $row->price\n";
    }
} catch (MongoDB\Driver\Exception\Exception $e) {
    $filename = basename(__FILE__);
    echo "The $filename script has experienced an error.\n";
    echo "It failed with the following exception:\n";
    echo "Exception:", $e->getMessage(), "\n";
    echo "In file:", $e->getFile(), "\n";
    echo "On line:", $e->getLine(), "\n";
}

?>

filtering.php

<?php

try {

    $mng = new MongoDB\Driver\Manager("mongodb://localhost:27017");
    $filter = [ 'name' => 'Volkswagen' ];
    $query = new MongoDB\Driver\Query($filter);
    $res = $mng->executeQuery("testdb.cars", $query);
    $car = current($res->toArray());
    if (!empty($car)) {
        echo $car->name, ": ", $car->price, PHP_EOL;
    } else {
        echo "No match found\n";
    }
} catch (MongoDB\Driver\Exception\Exception $e) {
    $filename = basename(__FILE__);
    echo "The $filename script has experienced an error.\n";
    echo "It failed with the following exception:\n";
    echo "Exception:", $e->getMessage(), "\n";
    echo "In file:", $e->getFile(), "\n";
    echo "On line:", $e->getLine(), "\n";
}

?>

projection.php

<?php

try {

    $filter = [];
    $options = ["projection" => ['_id' => 0]];

    $mng = new MongoDB\Driver\Manager("mongodb://localhost:27017");
    $query = new MongoDB\Driver\Query($filter, $options);

    $rows = $mng->executeQuery("testdb.cars", $query);

    foreach ($rows as $row) {

           print_r($row);
    }

} catch (MongoDB\Driver\Exception\Exception $e) {

    $filename = basename(__FILE__);

    echo "The $filename script has experienced an error.\n";
    echo "It failed with the following exception:\n";

    echo "Exception:", $e->getMessage(), "\n";
    echo "In file:", $e->getFile(), "\n";
    echo "On line:", $e->getLine(), "\n";
}

?>

read_limit.php

<?php

try {

    $mng = new MongoDB\Driver\Manager("mongodb://localhost:27017");
    $query = new MongoDB\Driver\Query([], ['sort' => [ 'name' => 1], 'limit' => 5]);

    $rows = $mng->executeQuery("testdb.cars", $query);

    foreach ($rows as $row) {

        echo "$row->name : $row->price\n";
    }

} catch (MongoDB\Driver\Exception\Exception $e) {

    $filename = basename(__FILE__);

    echo "The $filename script has experienced an error.\n";
    echo "It failed with the following exception:\n";

    echo "Exception:", $e->getMessage(), "\n";
    echo "In file:", $e->getFile(), "\n";
    echo "On line:", $e->getLine(), "\n";
}

?>


bulkwrite.php

<?php
try {

    $mng = new MongoDB\Driver\Manager("mongodb://localhost:27017");
    $bulk = new MongoDB\Driver\BulkWrite;
    $doc = ['_id' => new MongoDB\BSON\ObjectID, 'name' => 'Toyota', 'price' => 26700];
    $bulk->insert($doc);
    $bulk->update(['name' => 'Audi'], ['$set' => ['price' => 52000]]);
    $bulk->delete(['name' => 'Hummer']);
    $mng->executeBulkWrite('testdb.cars', $bulk);
} catch (MongoDB\Driver\Exception\Exception $e) {
    $filename = basename(__FILE__);
    echo "The $filename script has experienced an error.\n";
    echo "It failed with the following exception:\n";
    echo "Exception:", $e->getMessage(), "\n";
    echo "In file:", $e->getFile(), "\n";
    echo "On line:", $e->getLine(), "\n";
}

?>


# sh generate 75GB oplog files
for i in {0..40}; do echo $i | head  -c 2146435072 /dev/zero/ > local.$i; done


-------------
####################################################################

sudo apt install docker.io
sudo docker pull mariadb

https://hub.docker.com/_/mariadb/
https://hub.docker.com/_/mariadb/

####################################################################

# Start a mariadb server instance
docker run --name some-mariadb -e MYSQL_ROOT_PASSWORD=root -d mariadb:latest

# Connect to MySQL from an application in another Docker container
docker run --name some-app --link some-mariadb:mysql -d application-that-uses-mysql

########################################################
# Connect to MariaDB from the MySQL command line client
sudo docker run -it --link some-mariadb:mysql --rm mariadb sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'

docker run -it --rm mariadb mysql -hsome.mysql.host -usome-mysql-user -p

# Container shell access and viewing MySQL logs
docker exec -it some-mariadb bash

# Using a custom MySQL configuration file
docker run --name some-mariadb -v /my/custom:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mariadb:tag

# Configuration without a cnf file
docker run --name some-mariadb -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mariadb:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci

# if you would like to see a complete list of available options, just run:
$ docker run -it --rm mariadb:tag --verbose --help

Creating database dumps
docker exec some-mariadb sh -c 'exec mysqldump --all-databases -uroot -p"$MYSQL_ROOT_PASSWORD"' > /some/path/on/your/host/all-databases.sql

-------------------------------------------------------------------------

sudo usermod -aG docker $USER

sudo docker images
sudo docker rmi 85d4ebaca2a6
sudo docker container ls
sudo docker ps
sudo docker volume create --name mariadb-data
sudo docker exec some-mariadb sh -c 'exec mysql -v  -uroot -p"$MYSQL_ROOT_PASSWORD" datagovro '  < contracte.sql
sudo docker exec some-mariadb env # get env in fo
sudo docker restart
sudo docker exec -it some-mariadb bash # go on bash
sudo docker run --rm -it 85d4ebaca2a6 /bin/bash
sudo docker inspect some-mariadb # get container info
sudo docker inspect some-mariadb | grep -i IPAddress
sudo docker stop 0ad4f8a7a832
sudo docker rm 0ad4f8a7a832
sudo docker run --rm -v /tmp:/tmp  -it 85d4ebaca2a6 ls -la /tmp # show remote host
sudo docker kill 318863c1ce1s

sudo docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container_name_or_id
sudo docker inspect --format '{{ .NetworkSettings.IPAddress }}' container_name_or_id
sudo docker inspect <container ID>
sudo docker inspect <container id> | grep "IPAddress"

sudo docker run --detach --rm -it 85d4ebaca2a6    bin/bash



----------------------------------------
// Copy to Docker Container
----------------------------------------
T1
sudo docker cp contracte.sql  hungry_heyrovsky:/tmp/contracte.sql
T2
sudo docker run --rm -it 85d4ebaca2a6 bin/bash
root@b64de510c39d:/#
root@b64de510c39d:/# find . -type f -name contracte.sql
./tmp/contracte.sql

----------------------------------------
// Copy from Docker Container
----------------------------------------
T1
root@b64de510c39d:/tmp# cp contracte.sql contracteDK.sql
T2
sudo docker cp  hungry_heyrovsky:/tmp/contracteDK.sql .



docker cp foo.txt mycontainer:/foo.txt
docker cp mycontainer:/foo.txt foo.txt



PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=0ad4f8a7a832
MYSQL_ROOT_PASSWORD=my-secret-pw
GOSU_VERSION=1.10
GPG_KEYS=199369E5404BD5FC7D2FE43BCBCB082A1BB943DB 	177F4010FE56CA3336300305F1656F24C74CD1D8 	430BDF5C56E7C94E848EE60C1C4CBDCDCD2EFD2A 	4D1BB29D63D98E422B2113B19334A25F8507EFA5
MARIADB_MAJOR=10.3
MARIADB_VERSION=1:10.3.9+maria~bionic
HOME=/root


Copying files from a Docker container to the host
docker run -d --name my-image-app myImage
docker cp my-image-app:/app/MyApp.dll /path/on/the/host/MyApp.dll
docker cp <containerId>:/file/path/within/container /host/path/target

Copying files from a Docker image to the host
docker run --rm -it --entrypoint cat myImage /app/MyApp.dll > /path/on/the/host/MyApp.dll

--------------

sudo docker run -it --link some-mariadb:mysql --rm mariadb sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'

sudo docker run -it --link some-mariadb:mysql --restart=unless-stopped --rm mariadb sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'

sudo docker run -it --link some-mariadb:mysql --rm mariadb sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD" --database datagovro'  -v /home/lime/Downloads:/home


GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'password' WITH GRANT OPTION;
FLUSH PRIVILEGES;

use mysql;
update user set password=PASSWORD("NEWPASSWORD") where User='root';
flush privileges;

telnet (mysql server ip) [portNo]
iptables -A INPUT -i eth0 -p tcp -m tcp --dport 3306 -j ACCEPT
service iptables stop


Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 8
Server version: 10.3.9-MariaDB-1:10.3.9+maria~bionic mariadb.org binary distribution
Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

show databases;
use datagovro;

https://andrewlock.net/handy-docker-commands-for-local-development-part-1/
https://blog.shanelee.name/2017/04/09/how-to-import-and-export-databases-in-mysql-or-mariadb-with-docker/
http://depressiverobot.com/2015/02/19/mysql-dump-docker.html
https://hub.docker.com/_/mysql/
https://hub.docker.com/_/mariadb/
https://forums.docker.com/t/how-to-make-mariadb-database-changes-persistent/17339
https://docs.docker.com/storage/volumes/#use-a-read-only-volume
https://gist.github.com/spalladino/6d981f7b33f6e0afe6bb
https://docs.docker.com/v17.09/engine/admin/volumes/bind-mounts/
https://docs.docker.com/samples/library/mysql/#connect-to-mysql-from-the-mysql-command-line-client
https://mariadb.com/kb/en/library/restoring-data-from-dump-files/
https://savaslabs.com/2017/02/15/docker-mysql-performance.html
https://mariadb.com/kb/en/library/backup-and-restore-overview/
https://blog.shanelee.name/2017/04/09/how-to-import-and-export-databases-in-mysql-or-mariadb-with-docker/
https://www.techrepublic.com/article/how-to-deploy-and-use-a-mysql-docker-container/
https://www.digitalocean.com/community/tutorials/how-to-import-and-export-databases-in-mysql-or-mariadb
https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#examples-for-official-repositories
https://www.digitalocean.com/community/tutorials/how-to-share-data-between-the-docker-container-and-the-host
https://docs.oracle.com/cd/E37670_01/E75728/html/section_x54_32w_gp.html
https://til.codes/copy-file-from-host-machine-to-docker-container/
https://medium.com/@gchudnov/copying-data-between-docker-containers-26890935da3f
https://www.shellhacks.com/docker-cp-command-copy-file-to-from-container/
https://docs.docker.com/engine/reference/commandline/cp/




mysql --user admin_restore --password < /data/backup/db1.sql  # import
mysql --user admin_restore_temp --password --force < /data/backup/db1.sql  # export



# Backup
docker exec CONTAINER /usr/bin/mysqldump -u root --password=root DATABASE > backup.sql

# Restore
cat backup.sql | docker exec -i CONTAINER /usr/bin/mysql -u root --password=root DATABASE



# https://www.shellhacks.com/mysql-show-users-privileges-passwords/
SELECT user,host,password FROM mysql.user;
SHOW GRANTS;


# remove slashes
sed 's:/.*::' file  > output_file
sed 's/\/.*//g' file_with_urls > output_file




docker run --name=nginx -d -v ~/nginxlogs:/var/log/nginx -p 5000:80 nginx

sudo docker run --rm -v /tmp:/tmp:rw  -it 85d4ebaca2a6 cp /tmp/123.txt /tmp/123.txt
sudo docker run --rm -v /tmp:/tmp:ro  -it 85d4ebaca2a6 cp /tmp/123.txt /tmp/123.txt


--------------------



docker run --name mariadb -e ALLOW_EMPTY_PASSWORD=yes bitnami/mariadb:latest





https://hub.docker.com/_/mariadb/

https://www.digitalocean.com/community/tutorials/how-to-import-and-export-databases-in-mysql-or-mariadb
https://mariadb.com/kb/en/library/importing-data-into-mariadb/
https://hub.docker.com/r/linuxserver/mariadb/
https://hub.docker.com/r/bitnami/mariadb/
https://severalnines.com/blog/mysql-docker-containers-understanding-basics
https://hub.docker.com/r/1and1internet/ubuntu-16-nginx-php-phpmyadmin-mariadb-10/
https://hub.docker.com/r/1and1internet/ubuntu-16-nginx-php-phpmyadmin-mariadb-10/
https://mariadb.com/kb/en/library/installing-and-using-mariadb-via-docker/
https://onap.readthedocs.io/en/amsterdam/submodules/so.git/docs/Working_with_SO_Docker.html
https://hub.docker.com/r/centos/mariadb-101-centos7/
https://hub.docker.com/r/linuxserver/mariadb/
https://hub.docker.com/r/linuxserver/mariadb/
docker run --name mariadb -e ALLOW_EMPTY_PASSWORD=yes bitnami/mariadb:latest


###################################################
https://www.fromdual.com/first-docker-steps-with-mysql-and-mariadb
docker --version
docker search mysql --no-trunc --filter=stars=100
docker login --username=fromdual
docker pull store/oracle/mysql-enterprise-server:5.7
docker images
docker pull mariadb:10.0
docker ps
docker run --name=mariadb  --detach --env=MYSQL_ROOT_PASSWORD=root  mariadb:latest
docker ps
docker logs mariadb
ps -ef | grep docker

--------------------------
bitnami

sudo docker run --name mariadb -e ALLOW_EMPTY_PASSWORD=yes bitnami/mariadb:latest
sudo -i
docker container ls
sudo docker run -it --link mariadb:mysql --rm mariadb sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'



docker pull 1and1internet/ubuntu-16-nginx-php-phpmyadmin-mariadb-10


UID=999
DB_PORT=3306
WEB_ADMIN_PORT=80
DB_ROOT="/my/local/path/"

docker run -u ${UID}:0 -p ${DB_PORT}:3306 -p ${WEB_ADMIN_PORT}:8080 -v ${DATABASE_ROOT}:/var/lib/mysql/ 1and1internet/ubuntu-16-nginx-php-phpmyadmin-mariadb-10





Mongo
http://l4wisdom.com/pymongo/mongoremove.php
https://www.w3schools.com/python/python_mongodb_delete.asp
https://www.w3schools.com/python/python_mongodb_drop_collection.asp
https://docs.mongodb.com/manual/tutorial/remove-documents/
https://docs.mongodb.com/manual/tutorial/query-documents/
https://docs.mongodb.com/manual/reference/mongo-shell/

py
https://automatetheboringstuff.com/chapter6/
https://www.python-kurs.eu/for-schleife.php
http://zetcode.com/lang/python/dictionaries/


Linux – MariaDB MySQL – Root Passwort vergessen, Root Passwort neu setzen
https://mariadb.com/kb/en/library/set-password/
https://kofler.info/root-login-problem-mit-mariadb/
https://der-linux-admin.de/2015/04/linux-mariadb-mysql-root-passwort-vergessen-root-passwort-neu-setzen/

systemctl stop mariadb.service
sudo mysql -u root

mysql -u root
mysql> use mysql;
mysql> update user set password=PASSWORD("neuespasswort") where User='root';
mysql> flush privileges;
mysql> exit

mysqladmin shutdown
systemctl start mariadb.service
mysqladmin -u root -p 'altes-password' password 'neues-password'


https://dev.mysql.com/doc/refman/8.0/en/show-tables.html
https://dev.mysql.com/doc/refman/8.0/en/show-tables.html
https://dev.mysql.com/doc/refman/8.0/en/extended-show.html
https://dev.mysql.com/doc/refman/8.0/en/show-columns.html
https://dev.mysql.com/doc/refman/8.0/en/show-table-status.html
https://mariadb.com/kb/en/library/optimize-table/
https://mariadb.com/kb/en/library/optimizing-tables/
https://mariadb.com/kb/en/library/analyze-table/
https://mariadb.com/kb/en/library/defragmenting-innodb-tablespaces/
https://mariadb.com/kb/en/library/histogram-based-statistics/
https://dev.mysql.com/doc/refman/8.0/en/optimize-table.html

SHOW TABLE STATUS FROM db_name;
SHOW COLUMNS FULL FROM tbl_name;
SHOW TABLES FROM database;
SHOW TABLES FULL FROM db_name;

OPTIMIZE TABLE tbl_name
ANALYZE TABLE tbl_name
ANALYZE TABLE tbl PERSISTENT FOR ALL;

mysqlcheck roundcube
mysqlcheck -o roundcube
mysqlcheck -o -A



############################################################################################






Import and Export Data
https://docs.mongodb.com/compass/master/import-export/
https://docs.mongodb.com/manual/reference/program/mongoimport/
https://docs.mongodb.com/manual/reference/program/mongoimport/#use
https://docs.mongodb.com/manual/reference/program/mongoexport/

--type <json|csv|tsv>

use test
db.traffic.insert( { _id: 1, volume: NumberLong('2980000'), date: new Date() } )


mongoimport --db users --collection contacts --file contacts.json
mongoexport --db users --collection contacts --out contacts.json


https://www.npmjs.com/package/json2csv-cli
https://www.npmjs.com/package/json2csv
https://www.techwalla.com/articles/how-to-convert-json-to-csv-in-python
http://blog.appliedinformaticsinc.com/how-to-parse-and-convert-json-to-csv-using-python/

sudo apt install npm -y
npm i json2csv
npm install json2csv
npm install -g json2csv
npm install json2csv --save
npm install json2csv-cli

which npm
/usr/bin/npm


json2csv -i input.json -f carModel,price,color -p
json2csv -i input.json -f carModel,price,color -o out.csv
json2csv -f price


# Initial creation of csv with headings
$ json2csv -i test.json -f name,version > test.csv
# Append additional rows
$ json2csv -i test.json -f name,version --no-header >> test.csv


Install CLI
npm install json2csv-cli -g
Install Api
npm install --save json2csv-cli

j2c --json [file.json] --csv [file.csv]


npm install -g npx
npx json2csv -i test.json -f name,version > test.csv


https://pypi.org/project/json_to_csv/
https://pypi.org/project/json2xml2csv/
https://pypi.org/project/json2csv/
pip install json2csv
pip install json2xml2csv
pip install json_to_csv

------------------------------------------------
PY
#import json
#json_parsed = json.loads(json_data)
------------------------------------------------
import json
import csv
employee_parsed = json.loads(employee_data)
emp_data = employee_parsed['employee_details']
# open a file for writing
employ_data = open('/tmp/EmployData.csv', 'w')
# create the csv writer object
csvwriter = csv.writer(employ_data)
count = 0
for emp in emp_data:
      if count == 0:
             header = emp.keys()
             csvwriter.writerow(header)
             count += 1
      csvwriter.writerow(emp.values())
employ_data.close()



------------------------------------------------
https://github.com/trizen/perl-scripts/blob/master/Converters/json2csv.pl
https://metacpan.org/pod/distribution/App-TextTableUtils/bin/json2csv




#!/usr/bin/perl

# Author: Daniel "Trizen" Șuteu
# Date: 08 March 2016
# License: GPLV3
# Website: https://github.com/trizen

# Converts a stream of newline separated json data to csv format.
# Related to: https://github.com/jehiah/json2csv

use 5.010;
use strict;
use warnings;

use Text::CSV qw();
use JSON qw(from_json);
use Getopt::Std qw(getopts);
use Text::ParseWords qw(quotewords);

use open IO => ':encoding(UTF-8)', ':std';

my %opt;
getopts('k:i:o:p:d:', \%opt);

my $in  = \*ARGV;
my $out = \*STDOUT;

if (defined($opt{i})) {
    open $in, '<', $opt{i}
      or die "Can't open file `$opt{i}' for reading: $!";
}

if (defined($opt{o})) {
    open $out, '>', $opt{o}
      or die "Can't open file `$opt{o}' for writing: $!";
}

sub usage {
    my ($code) = @_;
    print <<"EOT";
usage: $0 [options] [< input.json] [> output.csv]

options:
    -k fields.0,and,nested.fields,to,output
    -i /path/to/input.json (optional; default is stdin)
    -o /path/to/output.csv (optional; default is stdout)
    -d delimiter separator for csv (default: ",")
    -p print csv header row

example:
    $0 -k user.name,list.0,remote_ip -i input.json -o output.csv

EOT
    exit($code);
}

$opt{k} // usage(1);

sub unescape {
    my ($str) = @_;

    my %esc = (
               a => "\a",
               t => "\t",
               r => "\r",
               n => "\n",
               e => "\e",
               b => "\b",
               f => "\f",
              );

    $str =~ s{(?<!\\)(?:\\\\)*\\([@{[keys %esc]}])}{$esc{$1}}g;
    $str;
}

my @fields = map { [quotewords(qr/\./, 0, $_)] } quotewords(qr/\s*,\s*/, 1, $opt{k});

say($opt{p}) if defined($opt{p});

my $csv = Text::CSV->new(
                         {
                          eol      => "\n",
                          sep_char => defined($opt{d}) ? unescape($opt{d}) : ",",
                         }
                        )
  or die "Cannot use CSV: " . Text::CSV->error_diag();

sub extract {
    my ($json, $fields) = @_;

    my @row;
    foreach my $field (@{$fields}) {
        my $ref = $json;

        foreach my $key (@{$field}) {
            if (    ref($ref) eq 'ARRAY'
                and $key =~ /^[-+]?[0-9]+\z/
                and exists($ref->[$key])) {
                $ref = $ref->[$key];
            }
            elsif (ref($ref) eq 'HASH'
                   and exists($ref->{$key})) {
                $ref = $ref->{$key};
            }
            else {
                local $" = ' -> ';
                warn "[!] Field `$key' (from `@{$field}') does not exists in JSON.\n";
                $ref = undef;
                last;
            }
        }

        push @row, $ref;
    }

    \@row;
}

while (defined(my $line = <$in>)) {
    my $data = extract(from_json($line), \@fields);
    $csv->print($out, $data);
}




------------------------------------------------
# https://github.com/hashref/json2csv/blob/master/bin/json2csv
------------------------------------------------

#!/usr/bin/env perl

use strict;
use warnings;
use JSON;
use Text::CSV;

our $VERSION = '0.01';

my $in_file  = $ARGV[0];
my $out_file = $ARGV[0];
$out_file =~ s/\.\w+$/\.csv/;

my $csv = Text::CSV->new( { binary => 1, eol => "\n" } );
my $json = JSON->new->utf8();

my $count  = 0;
my @header = ();
my @out    = ();

open my $in, "<:encoding(utf8)", $in_file or die "$in_file: $!\n";
while ( my $row = <$in> ) {
    my $data = $json->decode($row);
    if ($data) {
        if ( $count++ == 0 ) {
            @header = keys %{$data};
            $csv->combine(@header);
            push @out, $csv->string();
        }

        my @array = ();
        foreach my $key (@header) {
            push @array, $data->{$key};
        }
        $csv->combine(@array);
        push @out, $csv->string();
    }
}
close($in);

open my $out, ">:encoding(utf8)", $out_file or die "$out_file: $!\n";
foreach my $line (@out) {
    print $out $line;
}
close($out);

--------------------------------------

#!/usr/bin/perl

use strict;
use warnings;
use JSON::PP;
use CGI;

my $json = JSON::PP->new->utf8;
#my $json_text = CGI->new->param('aws.json');
#my $perl_scalar = $json->decode( $json_text );

local $/;

open( my $fh, '<', 'aws.json' );
my $json_text = <$fh>;
my $perl_scalar = decode_json($json_text);

#print encode_json($perl_scalar);
print $json->utf8->encode($perl_scalar);

--------------------------------------

use strict;
use warnings;
use JSON;
use Data::Dumper;

local $/;
my $data = from_json(<DATA>);
print Dumper $data;

my @columns = qw ( col1 col2 col3 );

print join( ",", "key", @columns ), "\n";
foreach my $key ( sort keys %$data ) {
    print join( ",", $key, @{ $data->{$key} }{@columns} ), "\n";
}

--------------------------------------

#!/usr/bin/env perl
use strict;
use warnings;
use JSON;
use Text::CSV;
use Data::Dumper;

local $/;
my $data = from_json ( <DATA> );
print Dumper $data;

my $csv = Text::CSV -> new ( { 'binary' => 1 } );
my @columns = qw ( col1 col2 col3 );
$csv -> column_names ( @columns );

foreach my $key ( sort keys %$data ) {
    $csv -> print_hr ( \*STDOUT, $data->{$key} );
    print "\n";
}

foreach my $key ( sort keys %$data ) {
    my $row = [ $key, @{$data->{$key}}{@columns} ];
    $csv -> print ( \*STDOUT, $row );
    print "\n";
}





#############################################################
python
#############################################################
# Ingesting a CSV
# python generator_csv.py
# cat data.csv | psql -h localhost -p 5432 feeds -c "COPY blog_feed (author, content) FROM STDIN WITH (FORMAT CSV, HEADER TRUE);"

import csv
import datetime
from faker import Faker

fake = Faker()
MAX_RANGE = 1000000

with open('data.csv', 'w') as csvfile:
    field_names = ['author', 'content']
    writer = csv.DictWriter(csvfile, fieldnames=field_names)
    writer.writeheader()
    for i in range(0, MAX_RANGE):
        writer.writerow({
            'author': fake.name(),
            'content': fake.sentence(nb_words=16, variable_nb_words=True),
        })



# Ingesting JSON Data
# python generator_json.py
# cat data.json | psql -h localhost -p 5432 feeds -c "COPY news_feed (data) FROM STDIN;"
# https://info.crunchydata.com/blog/fast-csv-and-json-ingestion-in-postgresql-with-copy

import csv
import datetime
import json
import uuid
from faker import Faker

fake = Faker()
MAX_RANGE = 1000000
datetime_end = datetime.datetime.now()
datetime_start = datetime_end - datetime.timedelta(days=365)

# geneate JSON
with open('data.json', 'w') as f:
    for i in range(0, MAX_RANGE):
        f.write(json.dumps({
            'id': str(uuid.uuid4()),
            'author': fake.name(),
            'content': fake.sentence(nb_words=16, variable_nb_words=True),
            'source': fake.company(),
            'published_at': fake.date_time_between_dates(
                datetime_start=datetime_start,
                datetime_end=datetime_end,
            ).isoformat()
        }) + "\n")


import psycopg2
conn = psycopg2.connect("host=localhost dbname=postgres user=postgres")
cur = conn.cursor()
insert_query = "INSERT INTO users VALUES {}".format("(10, 'hello@dataquest.io', 'Some Name', '123 Fake St.')")
cur.execute(insert_query)
conn.commit()



# Ingesting JSON Data
# python generator_json.py
# cat data.json | psql -h localhost -p 5432 feeds -c "COPY news_feed (data) FROM STDIN;"
# https://info.crunchydata.com/blog/fast-csv-and-json-ingestion-in-postgresql-with-copy

import csv
import datetime
import json
import uuid
from faker import Faker

fake = Faker()
MAX_RANGE = 1000000
datetime_end = datetime.datetime.now()
datetime_start = datetime_end - datetime.timedelta(days=365)

# geneate JSON
with open('data.json', 'w') as f:
    for i in range(0, MAX_RANGE):
        f.write(json.dumps({
            'id': str(uuid.uuid4()),
            'author': fake.name(),
            'content': fake.sentence(nb_words=16, variable_nb_words=True),
            'source': fake.company(),
            'published_at': fake.date_time_between_dates(
                datetime_start=datetime_start,
                datetime_end=datetime_end,
            ).isoformat()
        }) + "\n")



http://initd.org/psycopg/docs/install.html#prerequisites
https://wiki.postgresql.org/wiki/Psycopg2_Tutorial
https://www.postgresql.org/docs/9.4/static/backup-dump.html
https://www.postgresql.org/docs/9.1/static/backup-dump.html
http://www.postgresqltutorial.com/postgresql-json/
http://www.postgresqltutorial.com/
http://www.postgresqltutorial.com/postgresql-insert/
http://www.postgresqltutorial.com/postgresql-update/
http://www.postgresqltutorial.com/postgresql-delete/
http://www.postgresqltutorial.com/postgresql-alter-table/
http://www.postgresqltutorial.com/postgresql-drop-table/
http://www.postgresqltutorial.com/postgresql-truncate-table/
https://www.interserver.net/tips/kb/export-import-postgresql-database/
https://www.a2hosting.com/kb/developer-corner/postgresql/import-and-export-a-postgresql-database
https://info.crunchydata.com/blog/fast-csv-and-json-ingestion-in-postgresql-with-copy
##########

https://zonca.github.io/2013/10/how-to-log-exceptions-in-python.html
https://code.tutsplus.com/tutorials/error-handling-logging-in-python--cms-27932


####################################################################
#
# Import MySQL character - Must be UTF-8 General CI
#
####################################################################

show character set;
SET NAMES 'utf8';

ALTER TABLE `mytable` CHANGE `UrbanRural` `UrbanRural` VARCHAR(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL;

truncate table mytable;
LOAD DATA LOCAL INFILE 'mytable.csv' INTO TABLE mytable CHARACTER SET 'utf8' FIELDS TERMINATED BY '","' LINES TERMINATED BY '\n';


UPDATE coduri_postale SET Judet = REPLACE (Judet, '"', '') WHERE 1;
UPDATE coduri_postale SET Denumire = REPLACE (Denumire, '"', '') WHERE 1;
UPDATE coduri_postale SET Localitati = REPLACE (Localitati, '"', '') WHERE 1;

SELECT * FROM coduri_postale WHERE Localitati LIKE 'L-V%';
UPDATE coduri_postale SET Program =  Localitati WHERE Localitati LIKE 'L-V%';
UPDATE coduri_postale SET Localitati =  NULL WHERE Localitati LIKE 'L-V%';


UPDATE coduri_postale SET Program = REPLACE (Program, '"', '') WHERE 1;
UPDATE coduri_postale SET Program = REPLACE (Program, ',', '') WHERE 1;




####################################################################
#
#
#
####################################################################

https://www.postgresql.org/docs/9.4/static/backup-dump.html
https://www.postgresql.org/docs/9.1/static/backup-dump.html
https://www.a2hosting.com/kb/developer-corner/postgresql/import-and-export-a-postgresql-database
https://www.interserver.net/tips/kb/export-import-postgresql-database/

pg_dump dbname > outfile
psql dbname < infile
psql --set ON_ERROR_STOP=on dbname < infile
pg_dump -h host1 dbname | psql -h host2 dbname

pg_dumpall > outfile
psql -f infile postgres
pg_dump dbname | gzip > filename.gz

pg_dump -U username dbname > dbexport.pgsql
pg_dump -U username dbname -N topology -T spatial_ref_sys > dbexport.pgsql
psql -U username dbname < dbexport.pgsql

pg_dump -U USERNAME DBNAME > dbexport.pgsql
psql -U USERNAME DBNAME < dbexport.pgsql

https://stackoverflow.com/questions/1120109/export-postgresql-table-to-csv-file-with-headings
https://wiki.postgresql.org/wiki/Psycopg2_Tutorial
https://stackoverflow.com/questions/3964681/find-all-files-in-a-directory-with-extension-txt-in-python
https://code.tutsplus.com/tutorials/error-handling-logging-in-python--cms-27932

COPY products_273 TO '/tmp/products_199.csv' DELIMITER ',' CSV HEADER;

psql -U user -d db_name -c "Copy (Select * From foo_table LIMIT 10) To STDOUT With CSV HEADER DELIMITER ',';" > foo_data.csv

psql dbname -F , --no-align -c "SELECT * FROM TABLE"

