/////////////////////////////////////////////////////////////////////////////
//
// Linux commands collection part1 ( www.commandlinefu.com )
//
// ---------------------------------------------------
// http://www.commandlinefu.com/commands/browse
//
/////////////////////////////////////////////////////////////////////////////

*
*
*
*
*
*

----------------------------------------

wget --random-wait -r -p -e robots=off -U mozilla http://www.example.com
#Download an entire website

wget -qO - "http://www.tarball.com/tarball.gz" | tar zxvf -
#Extract tarball from internet without local saving

wget http://www.youtube.com/watch?v=dQw4w9WgXcQ -qO- | sed -n "/fmt_url_map/{s/[\'\"\|]/\n/g;p}" | sed -n '/^fmt_url_map/,/videoplayback/p' | sed -e :a -e '$q;N;5,$D;ba' | tr -d '\n' | sed -e 's/\(.*\),\(.\)\{1,3\}/\1/' | wget -i - -O surprise.flv
#Download Youtube video with wget!

wget -r -l1 --no-parent -nH -nd -P/tmp -A".gif,.jpg" http://example.com/images
#Download all images from a site

wget -qO - http://infiltrated.net/blacklisted|awk '!/#|[a-z]/&&/./{print "iptables -A INPUT -s "$1" -j DROP"}'
#Block known dirty hosts from reaching your machine

wget $URL | htmldoc --webpage -f "$URL".pdf - ; xpdf "$URL".pdf &
#Save an HTML page, and covert it to a .pdf file

wget -qO - http://example.com/path/to/blah.tar.gz | tar xzf -
#download and unpack tarball without leaving it sitting on your hard drive

wget --recursive --page-requisites --convert-links www.moyagraphix.co.za
#Download an entire static website to your local machine

echo 'wget url' | at 01:00
#Schedule a download at a later time

wget --reject html,htm --accept pdf,zip -rl1 url
#get all pdf and zips from a website using wget

wget http://URL/FILE.tar.gz -O - | tar xfz -
#Download a file and uncompress it while it downloads

wget -q -O - `youtube-dl -b -g $url`| ffmpeg -i - -f mp3 -vn -acodec libmp3lame -| mpg123 -
#Play music from youtube without download

pronounce(){ wget -qO- $(wget -qO- "http://dictionary.reference.com/browse/$@" | grep 'soundUrl' | head -n 1 | sed 's|.*soundUrl=\([^&]*\)&.*|\1|' | sed 's/%3A/:/g;s/%2F/\//g') | mpg123 -; }
#Pronounce an English word using Dictionary.com

wget -r -l1 -H -t1 -nd -N -np -A.mp3 -erobots=off [url of website]
#Download all mp3's listed in an html page

url=http://www.youtube.com/watch?v=V5bYDhZBFLA; youtube-dl -b $url; mplayer $(ls ${url##*=}*| tail -n1) -ss 00:57 -endpos 10 -vo gif89a:fps=5:output=output.gif -vf scale=400:300 -nosound
#Create an animated gif from a Youtube video

wget -O - http://www.commandlinefu.com/commands/browse/rss 2>/dev/null | awk '/\s*<title/ {z=match($0, /CDATA\[([^\]]*)\]/, b);print b[1]} /\s*<description/ {c=match($0, /code>(.*)<\/code>/, d);print d[1]"\n"} '
#Echo the latest commands from commandlinefu on the console

wget -qO- icanhazip.com
#Get your external IP address without curl

wget -nv http://en.wikipedia.org/wiki/Linux -O- | egrep -o "http://[^[:space:]]*.jpg" | xargs -P 10 -r -n 1 wget -nv
#Parallel file downloading with wget

echo 'wget url' | at 12:00
#Download schedule

d="www.dafont.com/alpha.php?";for c in {a..z}; do l=`curl -s "${d}lettre=${c}"|sed -n 's/.*ge=\([0-9]\{2\}\).*/\1/p'`;for((p=1;p<=l;p++));do for u in `curl -s "${d}page=${p}&lettre=${c}"|egrep -o "http\S*.com/dl/\?f=\w*"`;do aria2c "${u}";done;done;done
#Fetch every font from dafont.com to current folder

wget -p --convert-links http://www.foo.com
#Wget Command to Download Full Recursive Version of Web Page


wget -S -O/dev/null "INSERT_URL_HERE" 2>&1 | grep Server
#Poke a Webserver to see what it's powered by.

translate() { lng1="$1";lng2="$2";shift;shift; wget -qO- "http://ajax.googleapis.com/ajax/services/language/translate?v=1.0&q=${@// /+}&langpair=$lng1|$lng2" | sed 's/.*"translatedText":"\([^"]*\)".*}/\1\n/'; }
#Google Translate

wget -e robots=off -E -H -k -K -p http://<page>
#Use wget to download one page and all it's requisites for offline viewing

wget -erobots=off --user-agent="Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3" -H -r -l2 --max-redirect=1 -w 5 --random-wait -PmyBooksFolder -nd --no-parent -A.pdf http://URL
#Download free e-books

trickle -d 60 wget http://very.big/file
#Limit bandwidth usage by any program

wget randomfunfacts.com -O - 2>/dev/null|grep \<strong\>|sed "s;^.*<i>\(.*\)</i>.*$;\1;"|cowsay -f tux
#Lets Tux say the random fact. [add it to .bashrc to see it in new terminal window]

wget -qO - http://www.commandlinefu.com/commands/random/plaintext | sed -n '1d; /./p'
#Retrieve a random command from the commandlinefu.com API

wget -r -A .pdf -l 5 -nH --no-parent http://example.com
#Get all files of particular type (say, PDF) listed on some wegpage (say, example.com)

wget -r ftp://user:pass@ftp.example.com
#Download an entire ftp directory using wget

display "$(wget -q http://dynamic.xkcd.com/comic/random/ -O - | grep -Po '(?<=")http://imgs.xkcd.com/comics/[^"]+(png|jpg)')"
#random xkcd comic

ID=52DnUo6wJto;mplayer -fs $(echo "http://youtube.com/get_video.php?&video_id=$ID$(wget -qO - 'http://youtube.com/watch?v='$ID | perl -ne 'print $1."&asv=" if /^.*(&t=.*?)&.*$/; print "&fmt=".$1 if /^.*&fmt_map=(22).*$/')")
#Stream YouTube URL directly to mplayer.

head -100000 /dev/urandom | strings|tr '[A-Z]' '[a-z]'|sort >temp.txt && wget -q http://www.mavi1.org/web_security/wordlists/webster-dictionary.txt -O-|tr '[A-Z]' '[a-z]'|sort >temp2.txt&&comm -12 temp.txt temp2.txt
#Look for English words in /dev/urandom

wget -b http://dl.google.com/android/android-sdk_r14-linux.tgz
#background a wget download

wget -S --spider http://osswin.sourceforge.net/ 2>&1 | grep Mod
#Find out how old a web page is

for i in {1..10};do wget $(wget -O- -U "" "http://images.google.com/images?imgsz=xxlarge&hl=en&q=wallpaper+HD&start=$(($RANDOM%900+100))" --quiet | grep -oe 'http://[^"]*\.jpg' | head -1);done
#download 10 random wallpapers from google

u=`curl -d 'dl.start=Free' $(curl $1|perl -wpi -e 's/^.*"(http:\/\/rs.*)" method.*$/$1/'|egrep '^http'|head -n1)|grep "Level(3) \#2"|perl -wpi -e 's/^.*(http:\/\/rs[^\\\\]*).*$/$1/'`;sleep 60;wget $u
#rapidshare download script in 200 characters

wget -q --user=<username> --password=<password> 'https://updates.opendns.com/nic/update?hostname=your_opendns_hostname&myip=your_ip' -O -
#Update your OpenDNS network ip

wget --spider -v http://www.server.com/path/file.ext
#use wget to check if a remote file exists

wget -O - -q icanhazip.com
#return external ip

wget -U "QuickTime/7.6.2 (qtver=7.6.2;os=Windows NT 5.1Service Pack 3)" `echo http://movies.apple.com/movies/someHDmovie_720p.mov | sed 's/\([0-9][0-9]\)0p/h\10p/'`
#Download Apple movie trailers

wget http://domain.com/file{1..100}
#download file1 file2 file3 file4 .... file 100

wget -q -O - http://someonewhocares.org/hosts/ | grep ^127 >> /etc/hosts
#Block the 6700 worst spamhosts

wget -qO - "http://ajax.googleapis.com/ajax/services/language/translate?langpair=|en&v=1.0&q=`xsel`" |cut -d \" -f 6
#translate what is in the clipboard in english and write it to the terminal

wget -U "Mozilla/5.0" -qO - "http://translate.google.com/translate_a/t?client=t&text=translation+example&sl=auto&tl=fr" | sed 's/\[\[\[\"//' | cut -d \" -f 1
#Google Translate

wget --spider $URL 2>&1 | awk '/Length/ {print $2}'
#Retrieve the size of a file on a server

expandurl() { curl -sIL $1 | grep ^Location; }
#Expand shortened URLs

wget -q http://xyz.gpg -O- | sudo apt-key add -
#add a gpg key to aptitute package manager in a ubuntu system

wget --save-cookies ~/.cookies/rapidshare --post-data "login=USERNAME&password=PASSWORD" -O - https://ssl.rapidshare.com/cgi-bin/premiumzone.cgi > /dev/null
#Download from Rapidshare Premium using wget - Part 1

wget --http-user=YourUsername --http-password=YourPassword http://YourWebsiteUrl:2082/getbackup/backup-YourWebsiteUrl-`date +"%-m-%d-%Y"`.tar.gz
#backup your entire hosted website using cPanel backup interface and wget

wget --server-response --spider http://www.example.com/
#Dump HTTP header using wget

mirror=ftp://somemirror.com/with/alot/versions/but/no/latest/link; latest=$(curl -l $mirror/ 2>/dev/null | grep util | tail -1); wget $mirror/$latest
#get the latest version

p=$(echo "hello world, how r u?"|sed 's/ /+/g');wget -U Mozilla -q -O - "$@" translate.google.com/translate_tts?tl=en\&q=$p|mpg123 -
#Google text-to-speech in mp3 format

wget -O - http://example.com/a.gz | tar xz
#Extract tarball from internet without local saving

mv ubuntu-10.04-rc-desktop-amd64.iso ubuntu-10.04-desktop-amd64.iso; i=http://releases.ubuntu.com/10.04/ubuntu-10.04-desktop-amd64.iso.zsync; while true; do if wget $i; then zsync $i; date; break; else sleep 30; fi; done
#Auto download Ubuntu 10.04 LTS with super fast zsync

wget -O LICENSE.txt http://www.gnu.org/licenses/gpl-3.0.txt
#Add a GPL license file to your project

wget -qO- www.commandlinefu.com/commands/by/PhillipNordwall | awk -F\> '/num-votes/{S+=$2; I++}END{print S/I}'
#Find a CommandlineFu users average command rating

apt-popcon() { (echo \#rank; apt-cache search "$@" |awk '$1 !~ /^lib/ {print " "$1" "}') |grep -Ff- <(wget -qqO- http://popcon.debian.org/by_inst.gz |gunzip); }
#Search for packages, ranked by popularity

wget `lynx -dump http://www.ebow.com/ebowtube.php | grep .flv$ | sed 's/[[:blank:]]\+[[:digit:]]\+\. //g'`
#Grab all .flv files from a webpage to the current working directory

wget <URL> -O- | wget -i -
#Donwload media from *.rm from an url of type htttp://.../*.ram

wget -t inf -k -r -l 3 -p -m http://apod.nasa.gov/apod/archivepix.html
#Mirror the NASA Astronomy Picture of the Day Archive

y=http://www.youtube.com;for i in $(curl -s $f|grep -o "url='$y/watch?v=[^']*'");do d=$(echo $i|sed "s|url\='$y/watch?v=\(.*\)&.*'|\1|");wget -O $d.flv "$y/get_video.php?video_id=$d&t=$(curl -s "$y/watch?v=$d"|sed -n 's/.* "t": "\([^"]*\)",.*/\1/p')";done
#Download Youtube Playlist

wget -O /dev/null http://www.google.com
#Run remote web page, but don't save the results

wget http://twitter.com/help/test.json -q -O -
#Ping Twitter to check if you can connect

url="$my_url";file=$(youtube-dl -s -e $url);wget -q -O - `youtube-dl -b -g $url`| ffmpeg -i - -f mp3 -vn -acodec libmp3lame - > "$file.mp3"
#Play music from youtube without download

currency_convert() { wget -qO- "http://www.google.com/finance/converter?a=$1&from=$2&to=$3&hl=es" | sed '/res/!d;s/<[^>]*>//g'; }
#Currency Conversion

wget -qO- ifconfig.me/ip
#What is my public IP-address?

sudo apt-add-repository 'deb http://archive.offensive-security.com pwnsauce main microverse macroverse restricted universe multiverse' && wget -q http://archive.offensive-security.com/backtrack.gpg -O- | sudo apt-key add -
#BackTrack Repos

mplayer $(wget -q -O - "http://europarse.real.com/hurl/gratishurl.ram?pid=eu_aljazeera&amp;file=al_jazeera_en_lo.rm" | sed -e 's#lo.rm#hi.rm#')
#Watch Al Jazeera Livestream directly in mplayer #jan25

wget -q -U "Mozilla/5.0" --post-file speech.flac --header="Content-Type: audio/x-flac; rate=16000" -O - "http://www.google.com/speech-api/v1/recognize?lang=en-us&client=chromium"
#Google voice recognition "API"

wget -U Mozilla -qO - "http://thepiratebay.org/search/your_querry_here/0/7/0" | grep -o 'http\:\/\/torrents\.thepiratebay\.org\/.*\.torrent'
#ThePirateBay.org torrent search

tpb() { wget -U Mozilla -qO - $(echo "http://thepiratebay.org/search/$@/0/7/0" | sed 's/ /\%20/g') | grep -o 'http\:\/\/torrents\.thepiratebay\.org\/.*\.torrent' | tac; }
#ThePirateBay.org torrent search

echo -n "search> ";read QUERY && wget -O - `wget -O - -U "Mozilla/5.0" "http://images.google.com/images?q=${QUERY}" 2>/dev/null |sed -e 's/","http/\n","http/g' |awk -F \" '{print $3}' |grep -i http: |head -1` > "$QUERY"
#"I Feel Lucky" for Google Images

wget -qO - http://myip.dk/ | egrep -m1 -o '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}'
#Get My Public IP Address

wget http://search.twitter.com/trends.json -O - --quiet | ruby -rubygems -e 'require "json";require "yaml"; puts YAML.dump(JSON.parse($stdin.gets))'
#Print trending topics on Twitter

wget -q http://xyz.gpg -O- | sudo apt-key add -
#add a gpg key to aptitute package manager in a ubuntu system

watch -n 10 "wget -q http://www.brillig.com/debt_clock -O - | grep debtiv.gif | sed -e 's/.*ALT=\"//' -e 's/\".*//' -e 's/ //g'"
#Watch the National Debt clock

for i in be bg cz de es fi fr hu it lv lu at pl pt ro sk si ; do echo -n "$i " ; wget -q -O - http://www.expansys.$i/d.aspx?i=196165 | grep price | sed "s/.*<p id='price'><strong>&euro; \([0-9]*[,.][0-9]*\).*/\1/g"; done
#Compare prices in euro of the HTC Desire on all the european websites of Expansys.
















